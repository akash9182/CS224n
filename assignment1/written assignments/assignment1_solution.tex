\documentclass[letter,12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsmath,amssymb,amsthm}
\RequirePackage{tikz}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{afterpage}

\renewcommand{\lstlistlistingname}{Code Listings} 
\renewcommand{\lstlistingname}{Code Listing} 
\definecolor{gray}{gray}{0.5} 
\definecolor{key}{rgb}{0,0.5,0} 
\lstnewenvironment{python}[1][]{ 
\lstset{
language=python,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, ), \{, \}, :, *, !},
keywordstyle=\color{blue},
stringstyle=\color{red},
showstringspaces=false,
emph={class, pass, in, for, while, if, is, elif, else, not, and, or,
def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{key},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
upquote=true,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray}\slshape,
frame=tb,
rulesepcolor=\color{blue},#1
}}{}


\usetikzlibrary{calc}
\RequirePackage{tkz-euclide}
\usetkzobj{all}

\setlength{\textheight}{8.5in}
\setlength{\headheight}{.25in}
\setlength{\headsep}{.25in}
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.75in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros

% Math Macros.  It would be better to use the AMS LaTeX package,
% including the Bbb fonts, but I'm showing how to get by with the most
% primitive version of LaTeX.  I follow the naming convention to begin
% user-defined macro and variable names with the prefix "my" to make it
% easier to distiguish user-defined macros from LaTeX commands.
%
\newcommand{\myN}{\hbox{N\hspace*{-.9em}I\hspace*{.4em}}}
\newcommand{\myZ}{\hbox{Z}^+}
\newcommand{\myR}{\hbox{R}}

\newcommand{\myfunction}[3]
{${#1} : {#2} \rightarrow {#3}$ }

\newcommand{\myzrfunction}[1]
{\myfunction{#1}{{\myZ}}{{\myR}}}


% Formating Macros
%

\newcommand{\myheader}[4]
{\vspace*{-0.5in}
\noindent
{#1} \hfill {#3}

\noindent
{#2} \hfill {#4}

\noindent
\rule[8pt]{\textwidth}{1pt}

\vspace{1ex} 
}  % end \myheader 

\newcommand{\myalgsheader}[0]
{\myheader{Stanford University, Department of Computer Science}
{Computer Science 224n}{Spring 2017}{Section 1}}

% Running head (goes at top of each page, beginning with page 2.
% Must precede by \pagestyle{myheadings}.
\newcommand{\myrunninghead}[2]
{\markright{{\it {#1}, {#2}}}}

\newcommand{\myrunningalgshead}[2]
{\myrunninghead{Computer Science 224n}{{#1}}}

\newcommand{\myrunninghwhead}[2]
{\myrunningalgshead{Solution to Assignment {#1}, Problem {#2}}}

\newcommand{\mytitle}[1]
{\begin{center}
{\large {\bf {#1}}}
\end{center}}

\newcommand{\myhwtitle}[3]
{\begin{center}
{\large {\bf Solution to Assignment {#1}, Problem {#2}}}\\
\medskip
{\it {#3}} % Name goes here
\end{center}}

\newcommand{\myhwintro}[3]
{\begin{center}
{\large {\bf Assignment {#1}, Problem {#2}}}\\
\medskip
{\it {#3}} % Name goes here
\end{center}}

\newcommand{\mysection}[1]
{\noindent {\bf {#1}}}
\newcommand{\solutionsAuthor}{Akash Rana}
%%%%%% Begin document with header and title %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\myalgsheader
\pagestyle{myheadings}
\thispagestyle{plain}
\setcounter{page}{1}
\myhwtitle{1}{1(a)}{\solutionsAuthor}

\bigskip


\noindent {\bf Softmax} Prove that $softmax$ is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$,
\begin{equation}
\textrm{softmax}({\bf x}) = \textrm{softmax}({\bf x }+ c),
\end{equation}
where $({\bf x} + c)$ means adding the constant $c$ to every dimension of ${\bf x}$.

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\begin{equation}
\textrm{softmax}({\bf x})_{j} = \frac{e^{x_{j}}}{\sum_{k=1} {e^{x_{k}}}}
\end{equation}
\ Solution:
\begin{align}
\textrm{softmax}({\bf x} + c)_{j} & = \frac{e^{(x_{j} + c)}}{\sum_{k=1}
			 {e^{(x_{k} + c)}}}\\
			& = \frac{e^{c}} {e^{c}} \frac{e^{(x_{j})}}{sum_{k=1} {e^{(x_{j})}}}\\
			& = \textrm{softmax}({\bf x})_{j}
\end{align}


\clearpage
\pagestyle{myheadings}
\myrunninghwhead{1}{1 (softmax)}

\myhwtitle{1}{1(b)}{\solutionsAuthor}
\bigskip

\noindent Given an input matrix of \texttt{N}-rows and \texttt{d}-columns, compute the softmax prediction for each row.
Write your implementation in \texttt{q1\_softmax.py}. You may test by executing python \texttt{q1\_softmax.py}.\\

\noindent \textbf{Note:} The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is
important to have a correct implementation. Your implementation should also be efficient and vectorized
whenever possible. A non-vectorized implementation will not receive full credit!\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}

\begin{python}
import numpy as np

def softmax(x):
#x.shape by default gives column value
    if len(x.shape) > 1:
        # Matrix
        ### YOUR CODE HERE
        c = np.max(x, axis=1).reshape(-1,1) #-1 here means, internally  numpy is just calculating, to get the missing dimension.
        x = np.exp(x-c) / np.sum(np.exp(x-c), axis=1).reshape(-1,1)
        print (np.shape(x)) 
        ### END YOUR CODE
    else:
        # Vector
        ### YOUR CODE HERE
        c = np.max(x)
        x = np.exp(x-c) / np.sum(np.exp(x-c))
        ### END YOUR CODE

    assert x.shape == orig_shape
    return x
\end{python}

\clearpage 

\myrunninghwhead{1}{2 (Neural Networks)}

\myhwtitle{1}{2(a)}{\solutionsAuthor}

\bigskip

\noindent Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the
function value (i.e. in some expression where only $\sigma(x)$, but not $x$, is present). Assume that the input
$x$ is a scalar for this question.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\noindent Denote the sigmoid function as $\sigma(z)$,
\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}},
\end{equation}

Using chain rule, 

\begin{align*}
\sigma^{\prime}(z)   &= \frac{-1}{(1+e^{-z})^{2}}\times(-e^{-z})\\
                                 &= \(\frac{1}{1 + e^{-z}} \left(\frac{e^{-z}}{1 + e^{-z}} \right) \\
                                 &= \left(\frac{1}{1 + e^{-z}} \right) \left(\frac{1 + e^{-z}} {1 + e^{-z}} - \frac{1}{1 + e^{-z}} \right) \\
                                 &= \sigma(z)(1 - \sigma(z))
\end{align*}                                 

\clearpage

\myhwtitle{1}{2(b)}{\solutionsAuthor}
\bigskip

\noindent Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for
evaluation, i.e. find the gradients with respect to the softmax input vector $\theta$, when the prediction is
made by $\hat{y} = \textrm{softmax}(\theta)$. Remember the cross entropy function is
\begin{equation}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}
where $y$ is the one-hot label vector, and $\hat{y}$ is the predicted probability vector for all classes. \vspace{2mm}\\
\noindent {\bf Hint}: you might want to consider the fact many elements of $y$ are zeros, and assume that only the $k$-th dimension
of $y$ is one.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

Starting with, cross entropy


















\end{document}