\documentclass[letter,12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsmath,amssymb,amsthm}
\RequirePackage{tikz}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{afterpage}

\renewcommand{\lstlistlistingname}{Code Listings} 
\renewcommand{\lstlistingname}{Code Listing} 
\definecolor{gray}{gray}{0.5} 
\definecolor{key}{rgb}{0,0.5,0} 
\lstnewenvironment{python}[1][]{ 
\lstset{
language=python,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, ), \{, \}, :, *, !},
keywordstyle=\color{blue},
stringstyle=\color{red},
showstringspaces=false,
emph={class, pass, in, for, while, if, is, elif, else, not, and, or,
def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{key},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
upquote=true,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray}\slshape,
frame=tb,
rulesepcolor=\color{blue},#1
}}{}


\usetikzlibrary{calc}
\RequirePackage{tkz-euclide}
\usetkzobj{all}

\setlength{\textheight}{8.5in}
\setlength{\headheight}{.25in}
\setlength{\headsep}{.25in}
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.75in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros

% Math Macros.  It would be better to use the AMS LaTeX package,
% including the Bbb fonts, but I'm showing how to get by with the most
% primitive version of LaTeX.  I follow the naming convention to begin
% user-defined macro and variable names with the prefix "my" to make it
% easier to distiguish user-defined macros from LaTeX commands.
%
\newcommand{\myN}{\hbox{N\hspace*{-.9em}I\hspace*{.4em}}}
\newcommand{\myZ}{\hbox{Z}^+}
\newcommand{\myR}{\hbox{R}}

\newcommand{\myfunction}[3]
{${#1} : {#2} \rightarrow {#3}$ }

\newcommand{\myzrfunction}[1]
{\myfunction{#1}{{\myZ}}{{\myR}}}


% Formating Macros
%

\newcommand{\myheader}[4]
{\vspace*{-0.5in}
\noindent
{#1} \hfill {#3}

\noindent
{#2} \hfill {#4}

\noindent
\rule[8pt]{\textwidth}{1pt}

\vspace{1ex} 
}  % end \myheader 

\newcommand{\myalgsheader}[0]
{\myheader{Stanford University, Department of Computer Science}
{Computer Science 224n}{Spring 2017}{Section 1}}

% Running head (goes at top of each page, beginning with page 2.
% Must precede by \pagestyle{myheadings}.
\newcommand{\myrunninghead}[2]
{\markright{{\it {#1}, {#2}}}}

\newcommand{\myrunningalgshead}[2]
{\myrunninghead{Computer Science 224n}{{#1}}}

\newcommand{\myrunninghwhead}[2]
{\myrunningalgshead{Solution to Assignment {#1}, Problem {#2}}}

\newcommand{\mytitle}[1]
{\begin{center}
{\large {\bf {#1}}}
\end{center}}

\newcommand{\myhwtitle}[3]
{\begin{center}
{\large {\bf Solution to Assignment {#1}, Problem {#2}}}\\
\medskip
{\it {#3}} % Name goes here
\end{center}}

\newcommand{\myhwintro}[3]
{\begin{center}
{\large {\bf Assignment {#1}, Problem {#2}}}\\
\medskip
{\it {#3}} % Name goes here
\end{center}}

\newcommand{\mysection}[1]
{\noindent {\bf {#1}}}
\newcommand{\solutionsAuthor}{Akash Rana}
%%%%%% Begin document with header and title %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\myalgsheader
\pagestyle{myheadings}
\thispagestyle{plain}
\setcounter{page}{1}
\myhwtitle{1}{1(a)}{\solutionsAuthor}

\bigskip


\noindent {\bf Softmax} Prove that $softmax$ is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$,
\begin{equation}
\textrm{softmax}({\bf x}) = \textrm{softmax}({\bf x }+ c),
\end{equation}
where $({\bf x} + c)$ means adding the constant $c$ to every dimension of ${\bf x}$.

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\begin{equation}
\textrm{softmax}({\bf x})_{j} = \frac{e^{x_{j}}}{\sum_{k=1} {e^{x_{k}}}}
\end{equation}
\ Solution:
\begin{align}
\textrm{softmax}({\bf x} + c)_{j} & = \frac{e^{(x_{j} + c)}}{\sum_{k=1}
			 {e^{(x_{k} + c)}}}\\
			& = \frac{e^{c}} {e^{c}} \frac{e^{(x_{j})}}{sum_{k=1} {e^{(x_{j})}}}\\
			& = \textrm{softmax}({\bf x})_{j}
\end{align}


\clearpage
\pagestyle{myheadings}
\myrunninghwhead{1}{1 (softmax)}

\myhwtitle{1}{1(b)}{\solutionsAuthor}
\bigskip

\noindent Given an input matrix of \texttt{N}-rows and \texttt{d}-columns, compute the softmax prediction for each row.
Write your implementation in \texttt{q1\_softmax.py}. You may test by executing python \texttt{q1\_softmax.py}.\\

\noindent \textbf{Note:} The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is
important to have a correct implementation. Your implementation should also be efficient and vectorized
whenever possible. A non-vectorized implementation will not receive full credit!\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}

\begin{python}
import numpy as np

def softmax(x):
#x.shape by default gives column value
    if len(x.shape) > 1:
        # Matrix
        ### YOUR CODE HERE
        c = np.max(x, axis=1).reshape(-1,1) #-1 here means, internally  numpy is just calculating, to get the missing dimension.
        x = np.exp(x-c) / np.sum(np.exp(x-c), axis=1).reshape(-1,1)
        print (np.shape(x)) 
        ### END YOUR CODE
    else:
        # Vector
        ### YOUR CODE HERE
        c = np.max(x)
        x = np.exp(x-c) / np.sum(np.exp(x-c))
        ### END YOUR CODE

    assert x.shape == orig_shape
    return x
\end{python}

\clearpage 

\myrunninghwhead{1}{2 (Neural Networks)}

\myhwtitle{1}{2(a)}{\solutionsAuthor}

\bigskip

\noindent Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the
function value (i.e. in some expression where only $\sigma(x)$, but not $x$, is present). Assume that the input
$x$ is a scalar for this question.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

\noindent Denote the sigmoid function as $\sigma(z)$,
\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}},
\end{equation}

Using chain rule, 

\begin{align*}
\sigma^{\prime}(z)   &= \frac{-1}{(1+e^{-z})^{2}}\times(-e^{-z})\\
                                 &= \(\frac{1}{1 + e^{-z}} \left(\frac{e^{-z}}{1 + e^{-z}} \right) \\
                                 &= \left(\frac{1}{1 + e^{-z}} \right) \left(\frac{1 + e^{-z}} {1 + e^{-z}} - \frac{1}{1 + e^{-z}} \right) \\
                                 &= \sigma(z)(1 - \sigma(z))
\end{align*}                                 

\clearpage

\myhwtitle{1}{2(b)}{\solutionsAuthor}
\bigskip

\noindent Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for
evaluation, i.e. find the gradients with respect to the softmax input vector $\theta$, when the prediction is
made by $\hat{y} = \textrm{softmax}(\theta)$. Remember the cross entropy function is
\begin{equation}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}
where $y$ is the one-hot label vector, and $\hat{y}$ is the predicted probability vector for all classes. \vspace{2mm}\\
\noindent {\bf Hint}: you might want to consider the fact many elements of $y$ are zeros, and assume that only the $k$-th dimension
of $y$ is one.\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}

Cross entropy error function for multi-class output,
\begin{equation}
\textrm{CE}({\bf y},{\bf\hat{y}}) = -\sum_{i}{y_{i}\log{\hat{y_{i}}}}
\end{equation}

Computing the gradient yields, 

\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}} = -\frac{y_{j}}{\hat{y}_i}\
\label{eq:2b 1}
\end{equation}


\begin{align}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} = & \frac{\partial(\textrm{CE})}{\partial{\hat{y}_{i}}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}} \\
                                                                        = & -\frac{y_{j}}{\hat{y}_{i}}\frac{\partial{\hat{y}_{i}}}{\partial{\theta_{k}}}
\end{align}

Calculating the partial derivative of $\hat{y_{i}}$ (derivation using the quotient rule):

\begin{align}
\text{if} \; i = k :& \frac{\partial y_i}{\partial \theta_i}= \frac{\partial  \frac{e^{\theta_i}}{\Sigma_\theta}}{\partial \theta_i} \\
												=& \frac{e^{\theta_i}\Sigma_\theta - e^{\theta_i}e^{\theta_i}}{\Sigma_\theta^2} \\
												=&\frac{e^{\theta_i}}{\Sigma_\theta}\frac{\Sigma_\theta - e^{\theta_i}}{\Sigma_\theta} \\
												=& \frac{e^{\theta_i}}{\Sigma_\theta}(1-\frac{e^{\theta_i}}{\Sigma_\theta}) =  \hat{y_i} (1 - \hat{y_i}) \label{eq:2b 2}
\end{align}
\begin{align}
												\text{if} \; i \neq k :& \frac{\partial y_i}{\partial \theta_j} =\frac{\partial \frac{e^{\theta_i}}{\Sigma_\theta}}{\partial \theta_j}\\
												= &\frac{0 - e^{\theta_i}e^{\theta_j}}{\Sigma_\theta^2} \\
												=& -\frac{e^{\theta_i}}{\Sigma_\theta} \frac{e^{\theta_j}}{\Sigma_\theta} \\
												=& -\hat{y_i} y_k \label{eq:2b 3}
\end{align}

Combining Equations~\ref{eq:2b 1},~\ref{eq:2b 2},~\ref{eq:2b 3}, yields
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{k}}} = \begin{cases}
-y_{j}(1 - \hat{y}_{k})&\text{ for }i=k \\
y_{j}\hat{y}_{k}&\text{ for }i\neq k
\end{cases}
\end{equation}
Requiring $y_{j}$ to be non-zero, imposes that the auxiliary condition, $k=j$ and $y_{j}=1$, hence it follows immediately,
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\theta_{j}}} = \begin{cases}
(\hat{y}_{j} - 1)&\text{ for }i=j\label{eq:2b case 1} \\
\hat{y}_{j}&\text{ for }i\neq j
\end{cases}
\end{equation}
Which is equivalent to
\begin{equation}
\frac{\partial(\textrm{CE})}{\partial{\boldsymbol\theta}} = \bf{\hat{y}} - \bf{y}
\end{equation}

\clearpage

\myhwtitle{1}{2(c)}{\solutionsAuthor}
\bigskip

\noindent Derive the gradients with respect to the inputs x to an one-hidden-layer neural network (that is, find
$\partial{J}/\partial{\bf x}$ where $J$ is the cost function for the neural network). The neural network employs sigmoid activation
function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is $y$,
and cross entropy cost is used. (feel free to use $\sigma^{\prime}(x)$ as the shorthand for sigmoid gradient, and feel free
to define any variables whenever you see fit). \vspace{5mm}\\

\noindent\rule{\textwidth}{0.4pt}\vspace{5mm}
In order to simplify the notation used to solve the problem, define the following terms:
\begin{align}
{\bf x}^{(2)} \equiv & \quad{\bf h} \\
{\bf z}_{i}     \equiv& \quad{\bf x^{(i)}\textrm{W}_{i} + b_{i}}
\end{align}
Now, to calculate $\partial{J}/\partial{\bf x^{1}}$, one can use the back propagation algorithm. Starting with the chain rule and then results from Question 2(b):

\begin{equation}
\frac{\partial{J}}{\partial{\bf x}^{(2)}}  = \left( \left(\frac{\partial{J}}{\partial{\bf z}_{2}}  \frac{\partial{\bf z}_{2}}{\partial{\bf x}^{(2)}}\right) . \frac{\partial{\bf x}^{(2)}}{\partial{\bf z}_{1}} \right) * \frac{\partial{\bf z}_{1}}{\partial{\bf x}^{(1)}}
\end{equation}

\begin{equation}
\frac{\partial{J}}{\partial{\bf z}_{2}} = \bf{\hat{y}} - \bf{y}
\end{equation}
and
\begin{equation}
\frac{\partial{\bf z}_{i}}{\partial{{\bf x}^{(i)}}} = {\bf \textrm{W}^{\top}_{i}}\label{eq: 2c 1}
\end{equation}
Sigmoid ($\sigma$) derivative can be found in Question 2(a), but we define:
\begin{equation}
\frac{\partial{{\bf x}^{(2)}}}{\partial{{\bf z}_{1}}}\equiv\sigma^{\prime}(z_{1})%{\bf x}^{(2)}\cdot(1  - {\bf x}^{(2)})
\end{equation}
Combining these, and using $\cdot$ to denote element-wise product:
\begin{equation}
\frac{\partial{J}}{\partial{z_{i}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})
\end{equation}
Finally, using the results from Equation~\ref{eq: 2c 1} (but for the first layer):
\begin{equation}
\frac{\partial{J}}{\partial{{\bf x}^{(1)}}} = (\bf{\hat{y}} - \bf{y}){\bf \textrm{W}^{\top}_{2}}\cdot\sigma^{\prime}(z_{1})\cdot{\bf \textrm{W}^{\top}_{1}}
\end{equation}

\clearpage

\myhwtitle{1}{2(d)}{\solutionsAuthor}
\bigskip
\noindent How many parameters are there in this neural network, assuming the input is $\textrm{D}_{x}$-dimensional,
the output is $\textrm{D}_{y}$-dimensional, and there are H hidden units?\vspace{5mm}

\noindent\rule{\textwidth}{0.4pt}

\noindent $\bf{\textrm{W}}_{1}$ must have dimensions:  $\textrm{D}_{x}\times\textrm{H}$. The bias ($\bf{b}_{1}$) for the first layer must have
dimensions $\textrm{H}$. Adding these two together, yields $(\textrm{D}_{x} + 1)\times\textrm{H}$. Proceeding to the second layer,
there must be $\textrm{H}\times\textrm{D}_{y}$ parameters associated with the weight matrix ${\bf\textrm{W}}_{2}$. The bias ($\bf{b}_{2}$)
for the second layer must have dimensions $\textrm{D}_{y}$ elements. This yields, 
\begin{equation}
(\textrm{D}_{x} + 1)\times\textrm{H} + \textrm{D}_{y}\times(\textrm{H}+1)
\end{equation}
weights, for each input vector of dimensions $\textrm{D}_{x}$.





\end{document}